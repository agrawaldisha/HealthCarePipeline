{"cells": [{"cell_type": "code", "execution_count": 21, "id": "bf592a12-03f8-4420-8970-01127d38307d", "metadata": {}, "outputs": [], "source": "# import all the modules \n\nfrom google.cloud import storage, bigquery\nimport pandas as pd\nfrom pyspark.sql import SparkSession \nimport datetime\nimport json"}, {"cell_type": "code", "execution_count": 22, "id": "5bb7b1e7-c414-488e-87b2-d46337719517", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/08/04 13:39:54 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n25/08/04 13:39:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n25/08/04 13:39:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n25/08/04 13:39:55 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "#Initialize GCS & Bigquery clients\nstorage_client = storage.Client()\nbq_client = bigquery.Client()\n\n# Initialze the SparkSession. \nspark = SparkSession.builder.appName(\"HospitalMySQLToLanding\").getOrCreate()"}, {"cell_type": "code", "execution_count": 26, "id": "5dfd1443-80a6-4f2a-95a1-121957d77eb9", "metadata": {}, "outputs": [], "source": "# SetUp Variables from source to sink \n\n# google Cloud Storage Configurations \nGCS_BUCKET = \"healthcare-bucket-192\"\nHOSPITAL_NAME= \"hospital-a\"\nLANDING_PATH= f\"gs://{GCS_BUCKET}/landing/{HOSPITAL_NAME}/\"\nARCHIEVE_PATH= f\"gs://{GCS_BUCKET}/landing/{HOSPITAL_NAME}/archive\"\nCONFIG_FILE_PATH= f\"gs://{GCS_BUCKET}/configs/load_config.csv\""}, {"cell_type": "code", "execution_count": 27, "id": "11e8ccf5-4fda-4973-9a3e-417fd08ff0af", "metadata": {}, "outputs": [], "source": "# Bigquery configurations\n\nBQ_PROJECT= \"gcpdataengineering-467713\"\nBQ_AUDIT_TABLE= f\"{BQ_PROJECT}.temp.audit_log\"\nBQ_LOG_TABLE= f\"{BQ_PROJECT}.temp.pipeline_logs\"\nBQ_TEMP_PATH= f\"{GCS_BUCKET}/temp/\""}, {"cell_type": "code", "execution_count": 28, "id": "a548a6b4-49b5-4f56-8b82-ac0b39c8b614", "metadata": {}, "outputs": [], "source": "# SQL COnfigurations\n\nMYSQL_CONFIG={\n    \"url\": \"jdbc:mysql://34.9.188.225:3306/hospital_a_db?useSSL=false&allowPublicKeyRetrieval=true\",\n    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n    \"user\": \"myuser\",\n    \"password\": \"Dishu_192\"\n}"}, {"cell_type": "code", "execution_count": 29, "id": "8c73bb5f-5d9d-462b-a0c6-07176bc636d8", "metadata": {}, "outputs": [], "source": "#Logging Mechanism \n\nlog_entries=[]\ndef log_event(event_type, message, table=None):\n    \"\"\"Log an event store it in the log list\"\"\"\n    log_entry={\n        \"timestamp\" : datetime.datetime.now().isoformat(),\n        \"event_type\" : event_type,\n        \"message\" : message,\n        \"table\" : table\n    }\n    log_entries.append(log_entry)\n    print(f\"[{log_entry['timestamp']}]{event_type}-{message}\")  #Print for visibility"}, {"cell_type": "code", "execution_count": 30, "id": "99c6cb2b-0394-49c1-8b1d-e0a9d30e276d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "[2025-08-04T13:40:13.320548]INFO- \u2705 Successfully read the log file\n+-------------------+-------------+------------+-----------+------------+---------+----------+\n|           database|   datasource|   tablename|   loadtype|   watermark|is_active|targetpath|\n+-------------------+-------------+------------+-----------+------------+---------+----------+\n|hospital-a-mysql-db|hospital_a_db|  encounters|Incremental|ModifiedDate|        1|hospital-a|\n|hospital-a-mysql-db|hospital_a_db|    patients|Incremental|ModifiedDate|        1|hospital-a|\n|hospital-a-mysql-db|hospital_a_db|transactions|Incremental|ModifiedDate|        1|hospital-a|\n|hospital-a-mysql-db|hospital_a_db|   providers|       Full|        null|        1|hospital-a|\n|hospital-a-mysql-db|hospital_a_db| departments|       Full|        null|        1|hospital-a|\n|hospital-b-mysql-db|hospital_b_db|  encounters|Incremental|ModifiedDate|        1|hospital-b|\n|hospital-b-mysql-db|hospital_b_db|    patients|Incremental|ModifiedDate|        1|hospital-b|\n|hospital-b-mysql-db|hospital_b_db|transactions|Incremental|ModifiedDate|        1|hospital-b|\n|hospital-b-mysql-db|hospital_b_db|   providers|       Full|        null|        1|hospital-b|\n|hospital-b-mysql-db|hospital_b_db| departments|       Full|        null|        1|hospital-b|\n+-------------------+-------------+------------+-----------+------------+---------+----------+\n\n"}], "source": "#read Config file and create a dataframe out of this \n\ndef read_config_file():\n    df=spark.read.csv(CONFIG_FILE_PATH,header=True)\n    log_event(\"INFO\",\" \u2705 Successfully read the log file\")\n    return df\n\nconfig_df=read_config_file()\nconfig_df.show()\n"}, {"cell_type": "code", "execution_count": 31, "id": "9052388b-64f3-4c8d-ada6-d28cd4a0f35e", "metadata": {}, "outputs": [], "source": "def move_existing_files_to_archive(table):\n    try:\n        prefix = f\"landing/{HOSPITAL_NAME}/{table}/\"\n        blobs = list(storage_client.bucket(GCS_BUCKET).list_blobs(prefix=prefix))\n        existing_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n        \n        if not existing_files:\n            log_event(\"INFO\", f\"No existing files for the table {table}\", table=table)\n            return\n        \n        for file in existing_files:\n            source_blob = storage_client.bucket(GCS_BUCKET).blob(file)\n\n            # Extract the date from filename pattern: table_ddmmyy.json\n            filename = file.split(\"/\")[-1]\n            date_str = filename.replace(f\"{table}_\", \"\").replace(\".json\", \"\")\n            \n            if len(date_str) != 6:\n                log_event(\"WARNING\", f\"Filename date format incorrect for: {filename}\", table=table)\n                continue\n\n            day, month, year = date_str[:2], date_str[2:4], date_str[4:]\n\n            # Archive path\n            archive_path = f\"landing/{HOSPITAL_NAME}/archive/{table}/{year}/{month}/{day}/{filename}\"\n            destination_blob = storage_client.bucket(GCS_BUCKET).blob(archive_path)\n\n            # Copy and delete\n            storage_client.bucket(GCS_BUCKET).copy_blob(source_blob, storage_client.bucket(GCS_BUCKET), destination_blob.name)\n            source_blob.delete()\n\n            log_event(\"INFO\", f\"Moved {file} to {archive_path}\", table=table)\n    \n    except Exception as e:\n        log_event(\"ERROR\", f\"Failed to move files to archive for table {table}: {str(e)}\", table=table)\n"}, {"cell_type": "code", "execution_count": 32, "id": "ce0b12ed-1e75-4e10-8d84-7a76d61fc448", "metadata": {}, "outputs": [], "source": "def get_latest_watermark(table_name):\n    try:\n        query = f\"\"\"\n            SELECT MAX(load_timestamp) AS latest_timestamp\n            FROM `{BQ_AUDIT_TABLE}`\n            WHERE tablename = '{table_name}' AND data_source = 'hospital_a_db'\n        \"\"\"\n        query_job = bq_client.query(query)\n        result = query_job.result()\n        \n        for row in result:\n            return row.latest_timestamp.strftime('%Y-%m-%d %H:%M:%S') if row.latest_timestamp else \"1900-01-01 00:00:00\"\n        \n        return \"1900-01-01 00:00:00\"\n    \n    except Exception as e:\n        log_event(\"ERROR\", f\"Failed to get watermark for {table_name}: {str(e)}\", table=table_name)\n        return \"1900-01-01 00:00:00\"\n"}, {"cell_type": "code", "execution_count": 33, "id": "880d3659-b576-4359-8b28-8c94d3ef3d14", "metadata": {}, "outputs": [], "source": "def extract_and_save_to_landing(table, load_type, watermark_col):\n    try:\n        # Get latest watermark if incremental\n        last_watermark = get_latest_watermark(table) if load_type.lower() == 'incremental' else None\n        log_event(\"INFO\", f\"Latest watermark for {table}: {last_watermark}\", table=table)\n        \n        # Build the query\n        query = f\"(SELECT * FROM {table}) AS t\" if load_type.lower() == \"full\" else \\\n                f\"(SELECT * FROM {table} WHERE {watermark_col} > '{last_watermark}') AS t\"\n        \n        # Read data from MySQL\n        df = (spark.read.format(\"jdbc\")\n              .option(\"url\", MYSQL_CONFIG[\"url\"])\n              .option(\"user\", MYSQL_CONFIG[\"user\"])\n              .option(\"password\", MYSQL_CONFIG[\"password\"])\n              .option(\"driver\", MYSQL_CONFIG[\"driver\"])\n              .option(\"dbtable\", query)\n              .load())\n\n        log_event(\"SUCCESS\", f\"Successfully extracted data from {table}\", table=table)\n        \n        # Prepare file path and upload to GCS\n        today = datetime.datetime.today().strftime('%d%m%y')\n        JSON_FILE_PATH = f\"landing/{HOSPITAL_NAME}/{table}/{table}_{today}.json\"\n\n        bucket = storage_client.bucket(GCS_BUCKET)\n        blob = bucket.blob(JSON_FILE_PATH)\n        # Using Pandas because spark gnerated success files and log files \n        blob.upload_from_string(df.toPandas().to_json(orient=\"records\", lines=True), content_type=\"application/json\")\n        \n        log_event(\"SUCCESS\", f\"JSON file successfully written to gs://{GCS_BUCKET}/{JSON_FILE_PATH}\", table=table)\n        \n        # Prepare and write audit entry to BigQuery\n        audit_df = spark.createDataFrame([\n            (\"hospital_a_db\", table, load_type, df.count(), datetime.datetime.now(), \"SUCCESS\")\n        ], [\"data_source\", \"tablename\", \"load_type\", \"record_count\", \"load_timestamp\", \"status\"])\n\n        (audit_df.write.format(\"bigquery\")\n         .option(\"table\", BQ_AUDIT_TABLE)\n         .option(\"temporaryGcsBucket\", GCS_BUCKET)\n         .mode(\"append\")\n         .save())\n\n        log_event(\"SUCCESS\", f\"Audit log updated for {table}\", table=table)\n\n    except Exception as e:\n        log_event(\"ERROR\", f\"Error processing {table}: {str(e)}\", table=table)\n\n        \n# Whats done is variables, config files, logging, exceptions, error handling, reading wroting from multiple sources "}, {"cell_type": "code", "execution_count": 34, "id": "c00bfb26-c271-46d9-8037-ce5e17a55be4", "metadata": {}, "outputs": [], "source": "def save_logs_to_gcs(log_entries):\n    \"\"\"Saving logs to GCS\"\"\"\n    log_filename = f\"pipeline_log_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.json\"\n    log_filepath = f\"temp/pipeline_logs/{log_filename}\"\n    \n    json_data = json.dumps(log_entries, indent=4)\n    \n    # Get GCS bucket and blob reference\n    bucket = storage_client.bucket(GCS_BUCKET)\n    blob = bucket.blob(log_filepath)\n    \n    # Upload JSON data as a file\n    blob.upload_from_string(json_data, content_type=\"application/json\")\n    \n    print(f\"Logs successfully saved to GCS at gs://{GCS_BUCKET}/{log_filepath}\")\n"}, {"cell_type": "code", "execution_count": 35, "id": "ddf8ac38-e132-41c1-b386-fe1fc39d8767", "metadata": {}, "outputs": [], "source": "def save_logs_to_bigquery():\n    \"\"\"Saving logs to BigQuery\"\"\"\n    if log_entries:\n        log_df = spark.createDataFrame(log_entries)\n        log_df.write.format(\"bigquery\") \\\n            .option(\"table\", BQ_LOG_TABLE) \\\n            .option(\"temporaryGcsBucket\", GCS_BUCKET) \\\n            .mode(\"append\") \\\n            .save()\n        print(\"Logs stored in BigQuery for future analysis\")\n"}, {"cell_type": "code", "execution_count": null, "id": "036be1e1-b1ce-4a22-a578-f66eb8f34634", "metadata": {}, "outputs": [], "source": "# Ingest only active tables from config for 'hospital_a_db'\n\nfor row in config_df.collect():\n    if row[\"is_active\"] == '1' and row[\"datasource\"] == 'hospital_a_db':\n        db, src, table, load_type, watermark, _, targetpath = row\n        \n        # Step 1: Move any previous JSON files to archive\n        move_existing_files_to_archive(table)\n        \n        # Step 2: Extract new data and write to landing zone\n        extract_and_save_to_landing(table, load_type, watermark)\n\nsave_logs_to_gcs ()\nsave_logs_to_bigquery()"}, {"cell_type": "code", "execution_count": null, "id": "aa35e04b-d479-4d3a-b6d2-b8bd6b54447a", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "9f28cb72-969e-41c9-961d-9016b2ee307b", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "869bde84-5096-4ab6-b7ad-5deec8509622", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}