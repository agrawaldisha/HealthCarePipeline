{"cells": [{"cell_type": "code", "execution_count": 21, "id": "d629e87c-c129-434a-b441-23b6a279683e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession, functions as f\nfrom pyspark.sql.functions import when\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Claims Ingestion\") \\\n    .getOrCreate()\n\n# Define source and sink variables\nBUCKET_NAME = \"healthcare-bucket-192\"\nCLAIMS_BUCKET_PATH = f\"gs://{BUCKET_NAME}/landing/claims/*.csv\"\nBQ_TABLE = \"gcpdataengineering-467713.bronze_dataset.claims\"\nTEMP_GCS_BUCKET = BUCKET_NAME\n\n# Read CSV files from GCS\nclaims_df = spark.read.csv(CLAIMS_BUCKET_PATH, header=True)\n\n# Add datasource column based on file path\nclaims_df = claims_df.withColumn(\n        \"datasource\", \n        when(f.input_file_name().contains(\"hospital2\"), \"hospb\")\n        .when(f.input_file_name().contains(\"hospital1\"), \"hospa\")\n        .otherwise(\"none\")\n    )\nclaims_df=claims_df.dropDuplicates()\n# Write to BigQuery\nclaims_df.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", BQ_TABLE) \\\n    .option(\"temporaryGcsBucket\", TEMP_GCS_BUCKET) \\\n    .mode(\"overwrite\") \\\n    .save()\n"}, {"cell_type": "code", "execution_count": 29, "id": "f0d01a3b-1409-4798-acc4-9e23d7ed68b4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------------------------------------------------------------+\n|file_name                                                         |\n+------------------------------------------------------------------+\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n|gs://healthcare-bucket-192/landing/claims/hospital2_claim_data.csv|\n+------------------------------------------------------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----+\n|datasource|count|\n+----------+-----+\n|     hospb|10000|\n|     hospa|10000|\n+----------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read files AND immediately extract file name\nclaims_df = spark.read.csv(CLAIMS_BUCKET_PATH, header=True) \\\n    .withColumn(\"file_name\", f.input_file_name())\n\n# Confirm file names\nclaims_df.select(\"file_name\").show(truncate=False)\n\n# Assign datasource based on file name\nclaims_df = claims_df.withColumn(\n    \"datasource\", \n    when(f.col(\"file_name\").contains(\"hospital2\"), \"hospb\")\n    .when(f.col(\"file_name\").contains(\"hospital1\"), \"hospa\")\n    .otherwise(\"none\")\n)\n\n# Drop duplicates\nclaims_df = claims_df.dropDuplicates()\n\n# Optional: Count by datasource\nclaims_df.groupBy(\"datasource\").count().show()\n\n# Write to BigQuery\nclaims_df.write \\\n    .format(\"bigquery\") \\\n    .option(\"table\", BQ_TABLE) \\\n    .option(\"temporaryGcsBucket\", TEMP_GCS_BUCKET) \\\n    .mode(\"overwrite\") \\\n    .save()\n"}, {"cell_type": "code", "execution_count": null, "id": "d867df76-cc38-48cc-b76b-07c19b9dc8cb", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}